sparseLdaModelChem <- sda(x=trainChem,
y =trainInjury,
lambda = 0.01,
stop = -73)
## the ridge parameter called lambda.
predictionSparseLDAChem <-  predict(sparseLdaModelChem,testChem)
confusionMatrix(data = predictionSparseLDAChem$class,
reference = testInjury)
#######################################################
########### Nearest Shrunken Centroids ###########
library(pamr)
nscGridChem <- data.frame(.threshold = seq(0,4, by=0.1))
set.seed(975)
nscTunedChem <- train(x = trainChem,
y = trainInjury,
method = "pam",
preProc = c("center", "scale"),
tuneGrid = nscGridChem,
metric = "Accuracy",
trControl = ctrl)
predictionNSCChem <-predict(nscTunedChem,testChem)
confusionMatrix(data =predictionNSCChem, reference = testInjury)
library(caret)
library(AppliedPredictiveModeling)
data(hepatic)
# use ?hepatic to see more details
library(MASS)
set.seed(975)
barplot(table(injury),col=c("yellow","red","green"), main="Class Distribution")
set.seed(975)
#------------------------------------------------------------------------
# Use the Chemical predictors:
#------------------------------------------------------------------------
# this gives removes near-zero variance
# this is a categorical predictor and should remove near zero variance for this data
zv_cols = nearZeroVar(chem)
noZVChem = chem[,-zv_cols]
#remove the correlation between the predictors
highCorChem<-findCorrelation(cor(noZVChem),cutoff = .75)
filteredCorChem <- noZVChem[,-highCorChem]
# splitting data into 75% and 25% based on injury response
set.seed(975)
trainingRows =  createDataPartition(injury, p = .75, list= FALSE)
trainChem <- filteredCorChem[trainingRows,]
testChem <- filteredCorChem[-trainingRows, ]
trainInjury <- injury[trainingRows]
testInjury <- injury[-trainingRows]
ctrl <- trainControl(summaryFunction = defaultSummary)
############ Logistic Regression Analysis #############
# logistic regression
library(caret)
set.seed(975)
lrChem <- train(x=trainChem,
y = trainInjury,
method = "multinom",
metric = "Accuracy",
trControl = ctrl)
predictionLRChem<-predict(lrChem,testChem)
confusionMatrix(data =predictionLRChem,
reference = testInjury)
#######################################################
############ Linear Discriminant Analysis #############
# LDA Analysis
library(MASS)
set.seed(975)
ldaChem <- train(x = trainChem,
y = trainInjury,
method = "lda",
preProc = c("center","scale"),
metric = "Accuracy",
trControl = ctrl)
predictionLDAChem <-predict(ldaChem,testChem)
confusionMatrix(data =predictionLDAChem,
reference = testInjury)
##########################################################################
############## Partial Least Squares Discriminant Analysis ###############
library(MASS)
set.seed(975)
plsChem <- train(x = trainChem,
y = trainInjury,
method = "pls",
tuneGrid = expand.grid(.ncomp = 1:1),
preProc = c("center","scale"),
metric = "Accuracy",
trControl = ctrl)
predictionPLSChem <-predict(plsChem,testChem)
confusionMatrix(data =predictionPLSChem,
reference = testInjury)
#######################################################
########### Penalized Models ###########
########### Penalized Models for Logistic Regression ###########
glmnGrid <- expand.grid(.alpha = c(0, .1, .2, .4),
.lambda = seq(.01, .2, length = 10))
set.seed(975)
glmnTunedChem <- train(x=trainChem,
y =trainInjury,
method = "glmnet",
tuneGrid = glmnGrid,
preProc = c("center", "scale"),
metric = "Accuracy",
trControl = ctrl)
predictionGlmnetChem <-  predict(glmnTunedChem,testChem)
confusionMatrix(data =predictionGlmnetChem,
reference = testInjury)
########### Penalized Models for LDA ###########
library(sparseLDA)
set.seed(975)
sparseLdaModelChem <- sda(x=trainChem,
y =trainInjury,
lambda = 0.01,
stop = -73)
## the ridge parameter called lambda.
predictionSparseLDAChem <-  predict(sparseLdaModelChem,testChem)
confusionMatrix(data = predictionSparseLDAChem$class,
reference = testInjury)
#######################################################
########### Nearest Shrunken Centroids ###########
library(pamr)
nscGridChem <- data.frame(.threshold = seq(0,4, by=0.1))
set.seed(975)
nscTunedChem <- train(x = trainChem,
y = trainInjury,
method = "pam",
preProc = c("center", "scale"),
tuneGrid = nscGridChem,
metric = "Accuracy",
trControl = ctrl)
predictionNSCChem <-predict(nscTunedChem,testChem)
library(AppliedPredictiveModeling)
library(mlbench)
library(caret)
library(earth)
library(MASS)
library(elasticnet)
library(lars)
library(pls)
library(doParallel)
library(nnet)
data(permeability)
cat("After Non-Zero Variance, number of predictors in fingerprints is 388: \n")
NZVfingerprints <- nearZeroVar(fingerprints)
noNZVfingerprints <- fingerprints[,-NZVfingerprints]
print(str(noNZVfingerprints))
cat("\n\n")
# stratified random sample splitting with 75% training and 25% testing
set.seed(12345)
trainingRows =  createDataPartition(permeability, p = .75, list= FALSE)
trainFingerprints <- noNZVfingerprints[trainingRows,]
trainPermeability <- permeability[trainingRows,]
testFingerprints <- noNZVfingerprints[-trainingRows,]
testPermeability <- permeability[-trainingRows,]
set.seed(12345)
ctrl <- trainControl(method = "repeatedcv", repeats=5, number = 4)
# # For neuralnetwork, find the correlation and delete the correlated data
tooHigh <- findCorrelation(cor(trainFingerprints), cutoff = .75)
#
# #  the tooHigh gives 99 correlated datas
trainXnnet = trainFingerprints[,-tooHigh]
testXnnet = testFingerprints[,-tooHigh]
#
# set.seed(12344)
nnetGrid <- expand.grid(.decay = c(0, 0.01, .1),
.size = c(1:10),
## The next option is to use bagging (see the
## next chapter) instead of different random
## seeds.
.bag = FALSE)
nnetTune <- train(trainXnnet, trainFat,
method = "avNNet",
tuneGrid = nnetGrid,
trControl = ctrl,
## Automatically standardize data prior to modeling
## and prediction
preProc = c("center", "scale"),
linout = TRUE,
trace = FALSE,
MaxNWts = 10 * (ncol(trainXnnet) + 1) + 10 + 1,
maxit = 500)
library(AppliedPredictiveModeling)
library(mlbench)
library(caret)
library(earth)
library(MASS)
library(elasticnet)
library(lars)
library(pls)
library(doParallel)
library(nnet)
data(permeability)
cat("After Non-Zero Variance, number of predictors in fingerprints is 388: \n")
NZVfingerprints <- nearZeroVar(fingerprints)
noNZVfingerprints <- fingerprints[,-NZVfingerprints]
print(str(noNZVfingerprints))
cat("\n\n")
# stratified random sample splitting with 75% training and 25% testing
set.seed(12345)
trainingRows =  createDataPartition(permeability, p = .75, list= FALSE)
trainFingerprints <- noNZVfingerprints[trainingRows,]
trainPermeability <- permeability[trainingRows,]
testFingerprints <- noNZVfingerprints[-trainingRows,]
testPermeability <- permeability[-trainingRows,]
set.seed(12345)
ctrl <- trainControl(method = "repeatedcv", repeats=5, number = 4)
# # For neuralnetwork, find the correlation and delete the correlated data
tooHigh <- findCorrelation(cor(trainFingerprints), cutoff = .75)
#
# #  the tooHigh gives 99 correlated datas
trainXnnet = trainFingerprints[,-tooHigh]
testXnnet = testFingerprints[,-tooHigh]
#
# set.seed(12344)
nnetGrid <- expand.grid(.decay = c(0, 0.01, .1),
.size = c(1:10),
## The next option is to use bagging (see the
## next chapter) instead of different random
## seeds.
.bag = FALSE)
nnetTune <- train(trainXnnet, testXnnet,
method = "avNNet",
tuneGrid = nnetGrid,
trControl = ctrl,
## Automatically standardize data prior to modeling
## and prediction
preProc = c("center", "scale"),
linout = TRUE,
trace = FALSE,
MaxNWts = 10 * (ncol(trainXnnet) + 1) + 10 + 1,
maxit = 500)
library(AppliedPredictiveModeling)
library(mlbench)
library(caret)
library(earth)
library(MASS)
library(elasticnet)
library(lars)
library(pls)
library(doParallel)
library(nnet)
data(permeability)
cat("After Non-Zero Variance, number of predictors in fingerprints is 388: \n")
NZVfingerprints <- nearZeroVar(fingerprints)
noNZVfingerprints <- fingerprints[,-NZVfingerprints]
print(str(noNZVfingerprints))
cat("\n\n")
# stratified random sample splitting with 75% training and 25% testing
set.seed(12345)
trainingRows =  createDataPartition(permeability, p = .75, list= FALSE)
trainFingerprints <- noNZVfingerprints[trainingRows,]
trainPermeability <- permeability[trainingRows,]
testFingerprints <- noNZVfingerprints[-trainingRows,]
testPermeability <- permeability[-trainingRows,]
set.seed(12345)
ctrl <- trainControl(method = "repeatedcv", repeats=5, number = 4)
# # For neuralnetwork, find the correlation and delete the correlated data
tooHigh <- findCorrelation(cor(trainFingerprints), cutoff = .75)
#
# #  the tooHigh gives 99 correlated datas
trainXnnet = trainFingerprints[,-tooHigh]
testXnnet = testFingerprints[,-tooHigh]
#
# set.seed(12344)
nnetGrid <- expand.grid(.decay = c(0, 0.01, .1),
.size = c(1:10),
## The next option is to use bagging (see the
## next chapter) instead of different random
## seeds.
.bag = FALSE)
nnetTune <- train(trainXnnet, testPermeability,
method = "avNNet",
tuneGrid = nnetGrid,
trControl = ctrl,
## Automatically standardize data prior to modeling
## and prediction
preProc = c("center", "scale"),
linout = TRUE,
trace = FALSE,
MaxNWts = 10 * (ncol(trainXnnet) + 1) + 10 + 1,
maxit = 500)
library(caret)
library(AppliedPredictiveModeling)
data(hepatic)
# use ?hepatic to see more details
library(MASS)
set.seed(975)
barplot(table(injury),col=c("yellow","red","green"), main="Class Distribution")
#------------------------------------------------------------------------
# Use the biological predictors:
#------------------------------------------------------------------------
#this gives Z114 predictor has zero-variance
nearZeroVar(bio)
#remove the Z114 predictor and then find the correlation between the predictors
noZVbio <- bio[,-114]
#remove the correlation between the predictors
highCorBio<-findCorrelation(cor(noZVbio),cutoff = .75)
filteredCorBio <- noZVbio[,-highCorBio]
# splitting data into 75% and 25% based on injury response
set.seed(975)
trainingRows =  createDataPartition(injury, p = .75, list= FALSE)
trainBio <- filteredCorBio[ trainingRows, ]
testBio <- filteredCorBio[-trainingRows, ]
trainInjury <- injury[trainingRows]
testInjury <- injury[-trainingRows]
ctrl <- trainControl(summaryFunction = defaultSummary)
############ Logistic Regression Analysis #############
# logistic regression
library(caret)
set.seed(975)
lrBio <- train(x=trainBio,
y = trainInjury,
method = "multinom",
metric = "Accuracy",
trControl = ctrl)
predictionLRBio<-predict(lrBio,testBio)
confusionMatrix(data =predictionLRBio,
reference = testInjury)
#######################################################
############ Linear Discriminant Analysis #############
# LDA Analysis
library(MASS)
set.seed(975)
ldaBio <- train(x = trainBio,
y = trainInjury,
method = "lda",
metric = "Accuracy",
trControl = ctrl)
library(mlbench)
library(caret)
library(earth)
library(doParallel)
library(nnet)
data(tecator)
colName = {}
for (i in 1:100){
colName[i]<- paste("X",i)
}
colnames(absorp)<-colName
# splitting data into 80% and 20% based on Fat Response
set.seed(12345)
trainingRows =  createDataPartition(endpoints[,2], p = .80, list= FALSE)
trainAbsorption <- absorp[ trainingRows, ]
testAbsorption <- absorp[-trainingRows, ]
trainFat <- endpoints[trainingRows, 2]
testFat <- endpoints[-trainingRows, 2]
ctrl <- trainControl(method = "repeatedcv", repeats=4)
# # For neuralnetwork, find the correlation and delete the correlated data
tooHigh <- findCorrelation(cor(trainAbsorption), cutoff = .80)
#  the tooHigh gives 99 correlated datas
trainXnnet1 = trainAbsorption[,-tooHigh]
testXnnet1 = testAbsorption[,-tooHigh]
set.seed(12344)
library(nnet)
library(caret)
# without using PCA
# to train in parallel to 5 processor
cl <- makePSOCKcluster(5)
registerDoParallel(cl)
nnetGrid1 <- expand.grid(.decay = c(0, 0.01, .1),
.size = c(1:10),
## The next option is to use bagging (see the
## next chapter) instead of different random
## seeds.
.bag = FALSE)
nnetTune1 <- train(trainAbsorption, trainFat,
method = "avNNet",
trControl = ctrl,
preProc = c("center", "scale"),
linout = TRUE,
trace = FALSE,
MaxNWts = 10 * (ncol(trainAbsorption) + 1) + 10 + 1,
maxit = 500,
tuneGrid = nnetGrid1)
library(NCmisc)
list.functions.in.file("/tmp/us-500-small.R")
list.functions.in.file("/tmp/us-500-small.R")
list.functions.in.file("/tmp/us-500-small.R")
getwd()
setwd(""/Users/diederik/Desktop/master-thesis/tools/containerit"")
setwd("/Users/diederik/Desktop/master-thesis/tools/containerit")
ls()
library(containerit)
my_dockerfile <- containerit::dockerfile(from = utils::sessionInfo())
rmd_dockerfile <- containerit::dockerfile(from = "demos.Rmd",
image = "rocker/verse:3.5.2",
maintainer = "o2r",
filter_baseimage_pkgs = TRUE)
my_dockerfile <- containerit::dockerfile(from = utils::sessionInfo())
rmd_dockerfile <- containerit::dockerfile(from = "demo.Rmd",
image = "rocker/r-apt:bionic",
maintainer = "o2r",
filter_baseimage_pkgs = TRUE)
print(rmd_dockerfile)
setwd("/Users/diederik/Desktop/master-thesis/tools/containerit/us500")
ls()
my_dockerfile <- containerit::dockerfile(from = utils::sessionInfo())
rmd_dockerfile <- containerit::dockerfile(from = "demo.Rmd",
image = "rocker/r-apt:bionic",
maintainer = "o2r",
filter_baseimage_pkgs = TRUE)
print(rmd_dockerfile)
setwd("/Users/diederik/Desktop/master-thesis/tools/containerit/data-filtering")
my_dockerfile <- containerit::dockerfile(from = utils::sessionInfo())
rmd_dockerfile <- containerit::dockerfile(from = "demo.Rmd",
image = "rocker/r-apt:bionic",
maintainer = "o2r",
filter_baseimage_pkgs = TRUE)
print(rmd_dockerfile)
setwd("/Users/diederik/Desktop/master-thesis/tools/containerit/community-indeces")
my_dockerfile <- containerit::dockerfile(from = utils::sessionInfo())
rmd_dockerfile <- containerit::dockerfile(from = "demo.Rmd",
image = "rocker/r-apt:bionic",
maintainer = "o2r",
filter_baseimage_pkgs = TRUE)
print(rmd_dockerfile)
setwd("/Users/diederik/Desktop/master-thesis/tools/containerit/community-matrix")
my_dockerfile <- containerit::dockerfile(from = utils::sessionInfo())
rmd_dockerfile <- containerit::dockerfile(from = "demo.Rmd",
image = "rocker/r-apt:bionic",
maintainer = "o2r",
filter_baseimage_pkgs = TRUE)
print(rmd_dockerfile)
setwd("/Users/diederik/Desktop/master-thesis/tools/containerit/size-class")
my_dockerfile <- containerit::dockerfile(from = utils::sessionInfo())
rmd_dockerfile <- containerit::dockerfile(from = "demo.Rmd",
image = "rocker/r-apt:bionic",
maintainer = "o2r",
filter_baseimage_pkgs = TRUE)
print(rmd_dockerfile)
setwd("/Users/diederik/Desktop/master-thesis/tools/containerit/size-density")
my_dockerfile <- containerit::dockerfile(from = utils::sessionInfo())
rmd_dockerfile <- containerit::dockerfile(from = "demo.Rmd",
image = "rocker/r-apt:bionic",
maintainer = "o2r",
filter_baseimage_pkgs = TRUE)
print(rmd_dockerfile)
setwd("/Users/diederik/Desktop/master-thesis/tools/containerit/traits-computation")
my_dockerfile <- containerit::dockerfile(from = utils::sessionInfo())
rmd_dockerfile <- containerit::dockerfile(from = "demo.Rmd",
image = "rocker/r-apt:bionic",
maintainer = "o2r",
filter_baseimage_pkgs = TRUE)
print(rmd_dockerfile)
setwd("/Users/diederik/Desktop/master-thesis/tools/containerit/community-indeces")
my_dockerfile <- containerit::dockerfile(from = utils::sessionInfo())
rmd_dockerfile <- containerit::dockerfile(from = "demo.Rmd",
image = "rocker/r-apt:bionic",
maintainer = "o2r",
filter_baseimage_pkgs = TRUE)
print(rmd_dockerfile)
setwd("/Users/diederik/Desktop/master-thesis/tools/containerit/community-matrix")
my_dockerfile <- containerit::dockerfile(from = utils::sessionInfo())
rmd_dockerfile <- containerit::dockerfile(from = "demo.Rmd",
image = "rocker/r-apt:bionic",
maintainer = "o2r",
filter_baseimage_pkgs = TRUE)
print(rmd_dockerfile)
setwd("/Users/diederik/Desktop/master-thesis/tools/containerit/data-filtering")
my_dockerfile <- containerit::dockerfile(from = utils::sessionInfo())
rmd_dockerfile <- containerit::dockerfile(from = "demo.Rmd",
image = "rocker/r-apt:bionic",
maintainer = "o2r",
filter_baseimage_pkgs = TRUE)
print(rmd_dockerfile)
setwd("/Users/diederik/Desktop/master-thesis/tools/containerit/size-class")
my_dockerfile <- containerit::dockerfile(from = utils::sessionInfo())
rmd_dockerfile <- containerit::dockerfile(from = "demo.Rmd",
image = "rocker/r-apt:bionic",
maintainer = "o2r",
filter_baseimage_pkgs = TRUE)
print(rmd_dockerfile)
setwd("/Users/diederik/Desktop/master-thesis/tools/containerit/size-density")
my_dockerfile <- containerit::dockerfile(from = utils::sessionInfo())
rmd_dockerfile <- containerit::dockerfile(from = "demo.Rmd",
image = "rocker/r-apt:bionic",
maintainer = "o2r",
filter_baseimage_pkgs = TRUE)
print(rmd_dockerfile)
setwd("/Users/diederik/Desktop/master-thesis/tools/containerit/traits-computation")
my_dockerfile <- containerit::dockerfile(from = utils::sessionInfo())
rmd_dockerfile <- containerit::dockerfile(from = "demo.Rmd",
image = "rocker/r-apt:bionic",
maintainer = "o2r",
filter_baseimage_pkgs = TRUE)
print(rmd_dockerfile)
setwd("/Users/diederik/Desktop/master-thesis/tools/containerit/us-500")
my_dockerfile <- containerit::dockerfile(from = utils::sessionInfo())
rmd_dockerfile <- containerit::dockerfile(from = "demo.Rmd",
image = "rocker/r-apt:bionic",
maintainer = "o2r",
filter_baseimage_pkgs = TRUE)
print(rmd_dockerfile)
setwd("/Users/diederik/Desktop/master-thesis/tools/containerit/bio-seq")
my_dockerfile <- containerit::dockerfile(from = utils::sessionInfo())
rmd_dockerfile <- containerit::dockerfile(from = "demo.Rmd",
image = "rocker/r-apt:bionic",
maintainer = "o2r",
filter_baseimage_pkgs = TRUE)
print(rmd_dockerfile)
setwd("/Users/diederik/Desktop/master-thesis/tools/containerit/mario-kart")
my_dockerfile <- containerit::dockerfile(from = utils::sessionInfo())
rmd_dockerfile <- containerit::dockerfile(from = "demo.Rmd",
image = "rocker/r-apt:bionic",
maintainer = "o2r",
filter_baseimage_pkgs = TRUE)
print(rmd_dockerfile)
setwd("/Users/diederik/Desktop/master-thesis/rdb/")
setwd("/Users/diederik/Desktop/master-thesis/tools/containerit/applied-pred-6.1")
my_dockerfile <- containerit::dockerfile(from = utils::sessionInfo())
rmd_dockerfile <- containerit::dockerfile(from = "demo.Rmd",
image = "rocker/r-ver:4.1.1",
maintainer = "o2r",
filter_baseimage_pkgs = TRUE)
print(rmd_dockerfile)
